name: ELT Pipeline

on:
  push:
    branches:
      - main
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:  # Allow manual trigger

env:
  PYTHON_VERSION: '3.13'
  # Airbyte Configuration (non-sensitive)
  AIRBYTE_URL: 'http://your-airbyte-host:8000/api'
  AIRBYTE_WORKSPACE_ID: 'your-workspace-id-here'
  AIRBYTE_CONNECTION_ID: 'your-connection-id-here'

jobs:
  # Job 1: Build and Push Docker Image
  build-docker:
    name: Build & Push Docker Image
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata (tags, labels)
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ghcr.io/${{ github.repository }}
          tags: |
            type=ref,event=branch
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          build-args: |
            DATABRICKS_HOST=${{ secrets.DATABRICKS_HOST }}
            DATABRICKS_HTTP_PATH=${{ secrets.DATABRICKS_HTTP_PATH }}
            DATABRICKS_TOKEN=${{ secrets.DATABRICKS_TOKEN }}
            AIRBYTE_CLIENT_ID=${{ secrets.AIRBYTE_CLIENT_ID }}
            AIRBYTE_CLIENT_SECRET=${{ secrets.AIRBYTE_CLIENT_SECRET }}

      - name: Output image information
        run: |
          echo "âœ… Docker image built and pushed successfully!"
          echo ""
          echo "Image tags:"
          echo "${{ steps.meta.outputs.tags }}"

  # Job 2: Run Data Ingestion (Airbyte Sync)
  data-ingestion:
    name: Airbyte Data Sync
    runs-on: ubuntu-latest
    needs: build-docker
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install requests

      - name: Trigger Airbyte sync
        env:
          AIRBYTE_CLIENT_ID: ${{ secrets.AIRBYTE_CLIENT_ID }}
          AIRBYTE_CLIENT_SECRET: ${{ secrets.AIRBYTE_CLIENT_SECRET }}
        run: |
          echo "ðŸ”„ Triggering Airbyte sync..."
          echo "Airbyte URL: ${{ env.AIRBYTE_URL }}"
          echo "Workspace ID: ${{ env.AIRBYTE_WORKSPACE_ID }}"
          echo "Connection ID: ${{ env.AIRBYTE_CONNECTION_ID }}"
          python data_ingestion/trigger_sync.py ${{ env.AIRBYTE_CONNECTION_ID }}
          echo "âœ… Airbyte sync triggered successfully!"

      - name: Wait for sync completion (optional)
        run: |
          echo "â³ Waiting for sync to complete (60 seconds)..."
          sleep 60
          echo "âœ… Sync wait period complete"

  # Job 3: Run dbt Transformations
  dbt-transform:
    name: dbt Transformations
    runs-on: ubuntu-latest
    needs: data-ingestion
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dbt dependencies
        run: |
          pip install dbt-core==1.10.4 dbt-databricks==1.10.4

      - name: Create dbt profiles.yml
        env:
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
          DATABRICKS_HTTP_PATH: ${{ secrets.DATABRICKS_HTTP_PATH }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
        run: |
          mkdir -p ~/.dbt
          cat > ~/.dbt/profiles.yml <<EOF
          dvd_rental:
            target: dev
            outputs:
              dev:
                type: databricks
                catalog: workspace
                schema: dvd_rental
                host: ${DATABRICKS_HOST}
                http_path: ${DATABRICKS_HTTP_PATH}
                token: ${DATABRICKS_TOKEN}
                threads: 4
          EOF
          echo "âœ… profiles.yml created"

      - name: Run dbt debug
        working-directory: data_transformation/dvd_rental
        run: |
          echo "ðŸ” Testing dbt connection..."
          dbt debug
          echo "âœ… dbt connection successful!"

      - name: Run dbt models
        working-directory: data_transformation/dvd_rental
        run: |
          echo "ðŸš€ Running dbt models..."
          dbt run
          echo "âœ… dbt models built successfully!"

      - name: Run dbt tests
        working-directory: data_transformation/dvd_rental
        run: |
          echo "ðŸ§ª Running dbt tests..."
          dbt test
          echo "âœ… All dbt tests passed!"

      - name: Generate dbt documentation
        working-directory: data_transformation/dvd_rental
        run: |
          echo "ðŸ“š Generating dbt documentation..."
          dbt docs generate
          echo "âœ… Documentation generated!"

      - name: Upload dbt artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dbt-artifacts
          path: |
            data_transformation/dvd_rental/target/
          retention-days: 30

  # Job 4: Summary Report
  pipeline-summary:
    name: Pipeline Summary
    runs-on: ubuntu-latest
    needs: [build-docker, data-ingestion, dbt-transform]
    if: always()

    steps:
      - name: Pipeline completion summary
        run: |
          echo "======================================"
          echo "ELT Pipeline Execution Summary"
          echo "======================================"
          echo ""
          echo "Build Docker: ${{ needs.build-docker.result }}"
          echo "Data Ingestion: ${{ needs.data-ingestion.result }}"
          echo "dbt Transform: ${{ needs.dbt-transform.result }}"
          echo ""
          if [[ "${{ needs.build-docker.result }}" == "success" ]] && \
             [[ "${{ needs.data-ingestion.result }}" == "success" || "${{ needs.data-ingestion.result }}" == "skipped" ]] && \
             [[ "${{ needs.dbt-transform.result }}" == "success" || "${{ needs.dbt-transform.result }}" == "skipped" ]]; then
            echo "âœ… Pipeline completed successfully!"
            exit 0
          else
            echo "âŒ Pipeline failed. Check logs above."
            exit 1
          fi

